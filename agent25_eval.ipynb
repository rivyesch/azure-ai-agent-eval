{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48010f06",
   "metadata": {},
   "source": [
    "### Initializing Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to agent, ID: asst_GHYn52a8aVGYJehtyWYjuDGw\n",
      "Created thread, ID: thread_EqEPlob7PsZqPGMIgZ0Z1nw0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.agents.models import ListSortOrder\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "project = AIProjectClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    endpoint=os.environ[\"AZURE_AI_PROJECT\"])\n",
    "\n",
    "agent_id = os.environ[\"AGENT_ID\"]\n",
    "agent = project.agents.get_agent(agent_id)\n",
    "print(f\"Connected to agent, ID: {agent.id}\")\n",
    "\n",
    "thread = project.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd761b7",
   "metadata": {},
   "source": [
    "### Conversation with Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040a0036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created message, ID: msg_DO06rmom28Bs4qlHLIdZk9wq\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "MessageRole.USER: What are the approved email domains for MS Teams verification when resetting a password?\n",
      "MessageRole.AGENT: Okay, no worries, I’ll just check our system for the approved email domains you can use for MS Teams verification during a password reset. Give me a sec while I look that up.\n",
      "MessageRole.AGENT: All good, I've found the info for you. The only approved email domains for MS Teams verification when resetting a password are:\n",
      "\n",
      "- @asahi.com.au\n",
      "- @asahi.com.nz\n",
      "- @asahibeverages.com\n",
      "\n",
      "So, any Teams message used for verification must come from one of those internal domains. Let me know if you need anything else!【5:13†KB0013699 Password and MFA Reset - Process Changes and Secure Verification 1.md】【5:2†KB0013703 Password & MFA Reset Process - End User Article.md】\n",
      "Run ID: run_skCnqavcps6lT3rgfKQDCKXY\n"
     ]
    }
   ],
   "source": [
    "MESSAGE = \"What are the approved email domains for MS Teams verification when resetting a password?\"\n",
    "\n",
    "message = project.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content= MESSAGE\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")\n",
    "\n",
    "run = project.agents.runs.create_and_process(\n",
    "    thread_id=thread.id,\n",
    "    agent_id=agent.id)\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "else:\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    messages = project.agents.messages.list(thread_id=thread.id, order=ListSortOrder.ASCENDING)\n",
    "\n",
    "    for message in messages:\n",
    "        if message.text_messages:\n",
    "            print(f\"{message.role}: {message.text_messages[-1].text.value}\")\n",
    "\n",
    "    # for message in project.agents.messages.list(thread.id, order=\"asc\"):\n",
    "    #     print(f\"Role: {message.role}\")\n",
    "    #     print(f\"Content: {message.content[0].text.value}\")\n",
    "    #     print(\"-\" * 40)\n",
    "\n",
    "print(f\"Run ID: {run.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae459f9",
   "metadata": {},
   "source": [
    "### Get data from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a962da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project)\n",
    "\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "# Get a single agent run data\n",
    "evaluation_data_single_run = converter.convert(thread_id=thread_id, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to save thread data to a JSONL file for evaluation\n",
    "# Save the agent thread data to a JSONL file\n",
    "\n",
    "import json\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=file_name)\n",
    "# print(json.dumps(evaluation_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba46e63",
   "metadata": {},
   "source": [
    "### Setting up evaluator\n",
    "\n",
    "We will select the following evaluators to assess the different aspects relevant for agent quality: \n",
    "\n",
    "- [Intent resolution](https://aka.ms/intentresolution-sample): measures the extent of which an agent identifies the correct intent from a user query. Scale: integer 1-5. Higher is better.\n",
    "- [Tool call accuracy](https://aka.ms/toolcallaccuracy-sample): evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps. Scale: float 0-1. Higher is better.\n",
    "- [Task adherence](https://aka.ms/taskadherence-sample): measures the extent of which an agent’s final response adheres to the task based on its system message and a user query. Scale: integer 1-5. Higher is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    "    RetrievalEvaluator,\n",
    "    DocumentRetrievalEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    ResponseCompletenessEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    QAEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    SexualEvaluator,\n",
    "    ViolenceEvaluator,\n",
    "    SelfHarmEvaluator,\n",
    "    ProtectedMaterialEvaluator,\n",
    "    IndirectAttackEvaluator,\n",
    "    DirectAttackEvaluator\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "# Needed to use content safety evaluators\n",
    "azure_ai_project = os.environ[\"AZURE_AI_PROJECT\"]\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "retrieval = RetrievalEvaluator(model_config=model_config)\n",
    "groundedness = GroundednessEvaluator(model_config=model_config)\n",
    "relevance = RelevanceEvaluator(model_config=model_config)\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config)\n",
    "\n",
    "coherence = CoherenceEvaluator(model_config=model_config)\n",
    "fluency = FluencyEvaluator(model_config=model_config)\n",
    "qa = QAEvaluator(model_config=model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc334db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The min and max of the label scores are inputs to document retrieval evaluator\n",
    "ground_truth_label_min = 0\n",
    "ground_truth_label_max = 4\n",
    "\n",
    "document_retrieval = DocumentRetrievalEvaluator(\n",
    "    # Specify the ground truth label range\n",
    "    ground_truth_label_min=ground_truth_label_min, \n",
    "    ground_truth_label_max=ground_truth_label_max,\n",
    "    # Optionally override the binarization threshold for pass/fail output\n",
    "    ndcg_threshold = 0.5,\n",
    "    xdcg_threshold = 50.0,\n",
    "    fidelity_threshold = 0.5,\n",
    "    top1_relevance_threshold = 50.0,\n",
    "    top3_max_relevance_threshold = 50.0,\n",
    "    total_retrieved_documents_threshold = 50,\n",
    "    total_ground_truth_documents_threshold = 50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4531f",
   "metadata": {},
   "source": [
    "### Run Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        # Layer 1: Agent evaluators\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "\n",
    "        # Layer 2: RAG evaluators\n",
    "        \"retrieval\": retrieval,\n",
    "        \"document_retrieval\": document_retrieval,\n",
    "        \"groundedness\": groundedness,\n",
    "        \"relevance\": relevance,\n",
    "        \"response_completeness\": response_completeness,\n",
    "\n",
    "        # Layer 3: General purpose\n",
    "        \"coherence\": coherence,\n",
    "        \"fluency\": fluency,\n",
    "        \"qa\": qa,\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea66b4",
   "metadata": {},
   "source": [
    "## Inspect results on Azure AI Foundry\n",
    "\n",
    "Go to AI Foundry URL for rich Azure AI Foundry data visualization to inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b63dfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can use the following to get the evaluation results in memory\n",
    "\n",
    "# average scores across all runs\n",
    "pprint(response[\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
